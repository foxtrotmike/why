{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "illusions.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/foxtrotmike/why/blob/master/illusions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93mU2nsV8uPx",
        "colab_type": "text"
      },
      "source": [
        "# Statistical Illusions\n",
        "## By Fayyaz Minhas\n",
        "### (Based on \"The Book of Why\" by Judea Pearl)\n",
        "\n",
        "You must have seen optical illusions that baffle and sometimes overwhelm our vision system. A simple Google search on the subject should give your head spinning so I will not post any optical illusions in this post! However, it should be noted that there is nothing optical about these illusions -- they are illusions because they fool our vision system that comprises of not just our optics (eyes) but also the associated neural interpreter (our brain). \n",
        "\n",
        "Similar to optical illusions, there can be illusions in data that can baffle our minds. Here is a simple experiment:\n",
        "\n",
        "1. We toss two unbiased coins a 1000 times\n",
        "2. We find the correlation between the number of times the coins (A and B) turn up Heads\n",
        "3. We find the correlation between the number of times the coins (A and B) turn up Heads if at least one of them comes out to be Heads\n",
        "\n",
        "Since the coins are independent, we expect no correlation for step 2. However, in step 3, most of the time coin A turns up heads, coin B will be tails and vice-versa. This gives rise to a significant correlation. Don't believe me? Let's try it out in a simple Python script."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-RoTGKgAMYi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e6346d56-774e-45cf-c083-28be513a36a1"
      },
      "source": [
        "import numpy as np\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "#Let's generate some coin tosses\n",
        "\n",
        "R = np.random.randint(0,2,size=(1000,2))\n",
        "\n",
        "print(\"Correlation between the two coins:\",pearsonr(R[:,0],R[:,1]))\n",
        "\n",
        "#Let's do it for the cases when at least one of the coins lands heads\n",
        "\n",
        "Z = R[np.sum(R,axis=1)>0,:]\n",
        "\n",
        "print(\"Correlation between the two coins when at least one lands heads:\",pearsonr(Z[:,0],Z[:,1]))\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correlation between the two coins: (0.01859010023571264, 0.5570787491606352)\n",
            "Correlation between the two coins when at least one is heads: (-0.5017087766756608, 4.7487699437112227e-48)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Onett-v2A137",
        "colab_type": "text"
      },
      "source": [
        "Where did this correlation (~-0.5) from? \n",
        "\n",
        "We know that there is no way the first coin and the second coin communicated to end up opposite in most cases. Also, there are no \"hidden variables\" or genies that might cause one of the coins to be \"flipped\" relative to the other. So what's going on?\n",
        "\n",
        "It's just an illusion caused by the simple fact that we conditioned the two coin outputs by ignoring all cases when both coins land tails! Here we do see a correlation but without any causation but it makes our minds go bonkers as we are trained to find causes even when none exist!\n",
        "\n",
        "A similar illusion called the \"birth weight paradox\" had baffled researchers in a more practical setting: Identifying the effect of maternal smoking on  mortality of infants with low birth weight. Contrary to apparent logic, among low birthweight infants of less than 2500 g, maternal smoking was known to be associated with lower infant mortality! In simpler words, the data pointed that an underweight infant appeared to survive better if the mother smoked!! However, it was simply the case of conditioning over birthweight while studying the impact of maternal smoking on infant mortality. You can read more about this in the awesome article [Commentary: Resolutions of the birthweight paradox: competing explanations and analytical insights](https://academic.oup.com/ije/article/43/5/1368/697084).\n",
        "\n",
        "Put simply, one shouldn't treat every correlation as important and it is important to understand how data is generated! In words of Judea Pearl:\n",
        "\n",
        "\"Much of this data-centric history still haunts us today. We live in an era that presumes Big Data to be the solution to all our problems. Courses in “data science” are proliferating in our universities, and jobs for “data scientists” are lucrative in the companies that participate in the “data economy.” But I hope with this book to convince you that data are profoundly dumb. Data can tell you that the people who took a medicine recovered faster than those who did not take it, but they can’t tell you why. Maybe those who took the medicine did so because they could afford it and would have recovered just as fast without it.\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-MOFbieAynU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}